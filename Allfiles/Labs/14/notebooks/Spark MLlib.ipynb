{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train a machine learning model with Spark MLlib\n",
        "\n",
        "Spark MLlib enables you to train, validate, and use machine learning models on Apache Spark.\n",
        "\n",
        "In this notebook, you'll implement a machine learning solution that predicts whether or not a flight will arrive late based on a number of factors, such as the day and time of the flight, the origin and destination airports, the carrier operating the flight, and any departure delays.\n",
        "\n",
        "> **Note**: This noteook is designed to be run in an Azure Synapse Analytics Spark pool.\n",
        "\n",
        "\n",
        "## Attach this notebook to a Spark pool\n",
        "\n",
        "To run the code in this notebook, you'll need to use a Spark pool; so at the top of this notebook, in the **Attach to** list, select your Spark pool."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore Data\r\n",
        "\r\n",
        "Before training a model, a data scientists generally explores the data with which the model will be trained, and prepares it by removing errors and outliers, identifying predictive features, and potentially using *feature engineering* techniques to augment the data with new derived values that will result in a more predictive model.\r\n",
        "\r\n",
        "In this example, you'll explore some historical flight data with which we'll later train a model to predict whether or not a flight will arrive late. Along the way, we'll encounter somecommon techniques used to load and explore data in Spark.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data Using an Explicit Schema\n",
        "\n",
        "Let's start by loading some historical flight data into a dataframe. If the structure of the data is known ahead of time, you can explicitly specify the schema for the dataframe.\n",
        "\n",
        "Review the code in the cell below, which defines a schema for flight data before loading it from a text file. Then click the **&#9655;** button to the left of the cell to run it.\n",
        "\n",
        "> **Note**: The first time you run a cell in a notebook, the Spark pool must be started; which can take a few minutes."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "flightSchema = StructType([\n",
        "  StructField(\"DayofMonth\", IntegerType(), False),\n",
        "  StructField(\"DayOfWeek\", IntegerType(), False),\n",
        "  StructField(\"Carrier\", StringType(), False),\n",
        "  StructField(\"OriginAirportID\", IntegerType(), False),\n",
        "  StructField(\"DestAirportID\", IntegerType(), False),\n",
        "  StructField(\"DepDelay\", IntegerType(), False),\n",
        "  StructField(\"ArrDelay\", IntegerType(), False),\n",
        "])\n",
        "\n",
        "flights = spark.read.csv('/data/raw-flight-data.csv', schema=flightSchema, header=True)\n",
        "display(flights.limit(20))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Infer a Data Schema\n",
        "If the structure of the data source is unknown, you can have Spark automatically infer the schema.\n",
        "\n",
        "In this case, you will load data about airports without knowing the schema.\n",
        "\n",
        "Run the following cell to load airport data from a text file, inferring the column names and data types automatically."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "airports = spark.read.csv('/data/airports.csv', header=True, inferSchema=True)\n",
        "display(airports.limit(20))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Dataframe Methods\n",
        "Spark DataFrames provide functions that you can use to extract and manipulate data. For example, you can use the **select** function to return a new dataframe containing columns selected from an existing dataframe."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cities = airports.select(\"city\", \"name\")\n",
        "display(cities.limit(20))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine Operations\n",
        "You can combine functions in a single statement to perform multiple operations on a dataframe. In this case, you will use the **join** function to combine the **flights** and **airports** dataframes, and then use the **groupBy** and **count** functions to return the number of flights from each airport, and finally use the **orderBy** function to sort the results by the number of flights."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "flightsByOrigin = flights.join(airports, flights.OriginAirportID == airports.airport_id).groupBy(\"city\").count().orderBy(\"count\")\n",
        "display(flightsByOrigin.limit(30))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Count the Rows in a Dataframe\n",
        "Now that you're familiar with working with dataframes, a key task when building predictive solutions is to explore the data, determing statistics that will help you understand the data before building predictive models. For example, how many rows of flight data do you actually have?"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "flights.count()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determine the Presence of Duplicates\n",
        "The data you have to work with won't always be perfect - often you'll want to *clean* the data; for example to detect and remove duplicates that might affect your model. You can use the **dropDuplicates** function to create a new dataframe with the duplicates removed, enabling you to determine how many rows are duplicates of other rows."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "flights.count() - flights.dropDuplicates().count()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Identify Missing Values\n",
        "As well as determining if duplicates exist in your data, you should detect missing values, and either remove rows containing missing data or replace the missing values with a suitable relacement. The **dropna** function creates a dataframe with any rows containing missing data removed - you can specify a subset of columns, and whether the row should be removed in *any* or *all* values are missing. You can then use this new dataframe to determine how many rows contain missing values."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "flights.count() - flights.dropDuplicates().dropna(how=\"any\", subset=[\"ArrDelay\", \"DepDelay\"]).count()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean the Data\n",
        "Now that you've identified that there are duplicates and missing values, you can clean the data by removing the duplicates and replacing the missing values. The **fillna** function replaces missing values with a specified replacement value. In this case, you'll remove all duplicate rows and replace missing **ArrDelay** and **DepDelay** values with **0**."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "data=flights.dropDuplicates().fillna(value=0, subset=[\"ArrDelay\", \"DepDelay\"])\n",
        "data.count()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore the Data\n",
        "Now that you've cleaned the data, you can start to explore it and perform some basic analysis. Let's start by examining the lateness of a flight. The dataset includes the **ArrDelay** field, which tells you how many minutes behind schedule a flight arrived. However, if a flight is only a few minutes behind schedule, you might not consider it *late*. Let's make our definition of lateness such that flights that arrive within 25 minutes of their scheduled arrival time are considered on-time, but any flights that are more than 25 minutes behind schedule are classified as *late*. We'll add a column to indicate this classification:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.select(\"DayofMonth\", \"DayOfWeek\", \"Carrier\", \"OriginAirportID\",\"DestAirportID\",\n",
        "                   \"DepDelay\", \"ArrDelay\", ((col(\"ArrDelay\") > 25).cast(\"Int\").alias(\"Late\")))\n",
        "display(data.limit(20))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore Summary Statistics and Data Distribution\n",
        "Predictive modeling is based on statistics and probability, so we should take a look at the summary statistics for the columns in our data. The **describe** function returns a dataframe containing the **count**, **mean**, **standard deviation**, **minimum**, and **maximum** values for each numeric column."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "display(data.describe())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *DayofMonth* is a value between 1 and 31, and the mean is around halfway between these values; which seems about right. The same is true for the *DayofWeek* which is a value between 1 and 7. *Carrier* is a string, so there are no numeric statistics; and we can ignore the statistics for the airport IDs - they're just unique identifiers for the airports, not actually numeric values. The departure and arrival delays range between 63 or 94 minutes ahead of schedule, and over 1,800 minutes behind schedule. The means are much closer to zero than this, and the standard deviation is quite large; so there's quite a bit of variance in the delays. The *Late* indicator is a 1 or a 0, but the mean is very close to 0; which implies that there significantly fewer late flights than non-late flights.\n",
        "\n",
        "Let's verify that assumption by creating a table and using the **Spark SQL** API to run a SQL statement that counts the number of late and non-late flights:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "data.createOrReplaceTempView(\"flightData\")\n",
        "lateCounts = spark.sql(\"SELECT COUNT(*) AS Count, Late FROM flightData GROUP BY Late\")\n",
        "display(lateCounts)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it looks like there are significantly more non-late flights than late ones. You can see this more clearly with a visualization, so in the output above, select the **Chart** view to see the comparative counts as a bar chart.."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's address the outliers and imbalanced classes in our data by removing rows with extreme delay values, and *undersampling* the more common on-time flights:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand\n",
        "\n",
        "# Remove outliers - let's make the cut-off 150 minutes.\n",
        "data = data.filter(\"DepDelay < 150 AND ArrDelay < 150\")\n",
        "\n",
        "# Separate the late and on-time flights\n",
        "pos = data.filter(\"Late = 1\")\n",
        "neg = data.filter(\"Late = 0\")\n",
        "\n",
        "# undersample the most prevalent class to get a roughly even distribution\n",
        "posCount = pos.count()\n",
        "negCount = neg.count()\n",
        "if posCount > negCount:\n",
        "  pos = pos.sample(True, negCount/(negCount + posCount))\n",
        "else:\n",
        "  neg = neg.sample(True, posCount/(negCount + posCount))\n",
        "  \n",
        "# shuffle into random order (so a sample of the first 1000 has a mix of classes)\n",
        "data = neg.union(pos).orderBy(rand())\n",
        "\n",
        "# Replace the temporary table so we can query and visualize the balanced dataset\n",
        "data.createOrReplaceTempView(\"flightData\")\n",
        "\n",
        "# Show the statistics\n",
        "display(data.describe())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the maximums for the **DepDelay** and **ArrDelay** are clipped at under 150, and the mean value for the binary *Late* class is nearer 0.5; indicating a more or less even number of each class. We removed some data to accomplish this balancing act, but there are still a substantial number of rows for us to train a machine learning model with, and now the data is more balanced. Let's visualize the data again to confirm this:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "SELECT Late, COUNT(*) AS Count FROM flightData GROUP BY Late"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the data as a chart to compare the distribution of the **Late** classes as you did previously. There should now be a more even number of each class."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore Relationships in the Data\r\n",
        "Predictive modeling is largely based on statistical relationships between fields in the data. To design a good model, you need to understand how the data points relate to one another.\r\n",
        "\r\n",
        "A common way to start exploring relationships is to create visualizations that compare two or more data values. For example, run the following query to compare arrival delay by carrier.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "SELECT Carrier, ArrDelay FROM flightData"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "View the output as a chart, and then use the **View Options** button (which looks similar to **&#128463;<sub>*</sub>**) at the top-right of the output to configure the chart as follows:\r\n",
        "\r\n",
        "- **Chart type**: Box plot\r\n",
        "- **Key**: Carrier\r\n",
        "- **Values**: ArrDelay\r\n",
        "\r\n",
        "When you apply the view options, the box plots should should show that the median delay (the line in the middle of the box), and the distribution of delays varies by carrier; with some carriers having a higher median delay than others. The same is true for other features, such as the day of the week and the destination airport."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "You may already suspect that there's likely to be a relationship between departure delay and arrival delay, so let's examine that next. Run the next cell to retrieve the departure and arrival delays for each flight."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "SELECT DepDelay, ArrDelay FROM flightData"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "View the utput as a chart, and set the view options as follows:\r\n",
        "\r\n",
        "- **Chart type**: Scatter chart\r\n",
        "- **Key**: DepDelay\r\n",
        "- **Values**: ArrDelay\r\n",
        "- **Series Group**: *blank*\r\n",
        "- **Aggregation**: Avg\r\n",
        "\r\n",
        "The scatter plot shows the average arrival deplay for each recorded departure delay. Note that the points form a diagonal line, which indicates a strong linear relationship between departure delay and arrival delay.\r\n",
        "\r\n",
        "This linear relationship shows a *correlation* between these two values, which we can measure statistically. The **corr** function calculates a correlation value between -1 and 1, indicating the strength of correlation between two fields. A strong positive correlation (near 1) indicates that high values for one column are often found with high values for the other, which a strong negative correlation (near -1) indicates that *low* values for one column are often found with *high* values for the other. A correlation near 0 indicates little apparent relationship between the fields.\r\n",
        "\r\n",
        "Run the following cell to see the correlation statistic for these two variables."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.corr(\"DepDelay\", \"ArrDelay\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation is close to 1, indicating a reasonably strong positive correlation between departure delay and arrval delay. Flights that depart late, unsurprisingly, often arrive late!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train a machine learning model\r\n",
        "\r\n",
        "In a real scenario, a data scientist would explore the statistical distributions and relationships in the data in more depth, and perform *feature engineering* to prepare the dataset for training a machine learning model. In this exercise, we've provided a prepared version of the data for you. Use the following code to load it into a dataframe."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flightSchema = StructType([\r\n",
        "  StructField(\"DayofMonth\", IntegerType(), False),\r\n",
        "  StructField(\"DayOfWeek\", IntegerType(), False),\r\n",
        "  StructField(\"Carrier\", StringType(), False),\r\n",
        "  StructField(\"OriginAirportID\", IntegerType(), False),\r\n",
        "  StructField(\"DestAirportID\", IntegerType(), False),\r\n",
        "  StructField(\"DepDelay\", IntegerType(), False),\r\n",
        "  StructField(\"ArrDelay\", IntegerType(), False),\r\n",
        "  StructField(\"Late\", IntegerType(), False),\r\n",
        "])\r\n",
        "\r\n",
        "data = spark.read.csv('/data/flights.csv', schema=flightSchema, header=True)\r\n",
        "display(data.limit(20))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the training data\r\n",
        "\r\n",
        "It is common practice when building supervised machine learning models to split the source data, using some of it to train the model and reserving some to test the trained model. In this exercise, you will use 70% of the data for training, and reserve 30% for testing."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splits = data.randomSplit([0.7, 0.3])\r\n",
        "train = splits[0]\r\n",
        "test = splits[1]\r\n",
        "train_rows = train.count()\r\n",
        "test_rows = test.count()\r\n",
        "print (\"Training Rows:\", train_rows, \" Testing Rows:\", test_rows)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a model training pipeline\r\n",
        "\r\n",
        "A machine learning model is created by fitiing the training data to an algorithm that uses the input *features* to calculate a probably value for a predicted *label*. There are lots of available algorithms for specific kinds of machine learning model. or example, *classification* algorithms like *logistic regression* use features as inputs to calculate a probably *class* (or categorization); while *regression* algorithms, like *linear regression*, can be used to predict a numeric value. In this case, we're trying to predict whether a flight is going to be late or not, so this is an example of a *classification* problem. Specifically, since it requires a prediction based on two possible classes (essentially, *true* or *false*), it's an example of *binary classification*.\r\n",
        "\r\n",
        "However, while you can train a model by fitting your raw data to an algorithm, a good model often requires multiple stages of feature preparation. For example, it is common when using some algorithms to distingish between continuous features (which have a calculable numeric value) and categorical features (which are numeric representations of discrete categories). It is also common to *normalize* continuous numeric features to use a common scale - for example, by scaling all numbers to a proportional decimal value between 0 and 1 (strictly speaking, it only really makes sense to do this when you have multiple numeric columns - normalizing them all to similar scales prevents a feature with particularly large values from dominating the training of the model - in this case, we only have one non-categorical numeric feature; but we've included this so you can see how it's done!).\r\n",
        "\r\n",
        "A pipeline consists of a series of *transformer* and *estimator* components that typically prepare a dataframe for\r\n",
        "modeling and then train a predictive model. In this case, you will create a pipeline with the following components:\r\n",
        "- A **StringIndexer** estimator for each categorical variable to generate numeric indexes for categorical features\r\n",
        "- A **VectorAssembler** that creates a vector of continuous numeric features\r\n",
        "- A **MinMaxScaler** that normalizes vector of numeric features\r\n",
        "- A **VectorAssembler** that creates a vector of categorical and continuous features\r\n",
        "- A **LogisticRegression** algorithm that trains a classification model.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.ml import Pipeline\r\n",
        "from pyspark.ml.classification import LogisticRegression\r\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, MinMaxScaler\r\n",
        "\r\n",
        "# convert monthdays to indexed categories\r\n",
        "monthdayIndexer = StringIndexer(inputCol=\"DayofMonth\", outputCol=\"DayofMonthIdx\")\r\n",
        "# convert weekdays to indexed categories\r\n",
        "weekdayIndexer = StringIndexer(inputCol=\"DayOfWeek\", outputCol=\"DayOfWeekIdx\")\r\n",
        "# convert carriers to indexed categories\r\n",
        "carrierIndexer = StringIndexer(inputCol=\"Carrier\", outputCol=\"CarrierIdx\")\r\n",
        "# convert origins to indexed categories\r\n",
        "originIndexer = StringIndexer(inputCol=\"OriginAirportID\", outputCol=\"OriginAirportIdx\")\r\n",
        "# convert destinations to indexed categories\r\n",
        "destIndexer = StringIndexer(inputCol=\"DestAirportID\", outputCol=\"DestAirportIdx\")\r\n",
        "# Create a vector of all numeric variables (in this case, departure delay)\r\n",
        "numVect = VectorAssembler(inputCols = [\"DepDelay\"], outputCol=\"numFeatures\")\r\n",
        "# Normalize numerica variables\r\n",
        "minMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol=\"normNums\")\r\n",
        "#Create a single vector containing all feaures\r\n",
        "featVect = VectorAssembler(inputCols=[\"DayofMonthIdx\", \"DayOfWeekIdx\", \"CarrierIdx\", \"OriginAirportIdx\", \"DestAirportIdx\", \"normNums\"], outputCol=\"features\")\r\n",
        "# Use a logistic regression algorithm to predict the \"Late\" label from the vector of features\r\n",
        "lr = LogisticRegression(labelCol=\"Late\", featuresCol=\"features\")\r\n",
        "# Define a pipeline with all these steps in the right order\r\n",
        "pipeline = Pipeline(stages=[monthdayIndexer, weekdayIndexer, carrierIndexer, originIndexer, destIndexer, numVect, minMax, featVect, lr])\r\n",
        "print (\"Pipeline defined:\")\r\n",
        "print (pipeline.getStages())\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model\r\n",
        "\r\n",
        "The pipeline itself is an estimator, and so it has a **fit** method that you can call to run the pipeline on a specified dataframe. In this case, you will run the pipeline on the training data to train a model."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = pipeline.fit(train)\r\n",
        "print (\"Model trained!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the Model\r\n",
        "\r\n",
        "Now you're ready to use the **transform** method of the model to generate some predictions. You can use this approach to predict delay status for flights where the label is unknown; but in this case you are using the test data which includes a known true label value, so you can compare the predicted status to the actual status."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.transform(test)\r\n",
        "predicted = prediction.select(\"features\", \"probability\", col(\"prediction\").cast(\"Int\"), col(\"Late\").alias(\"trueLabel\"))\r\n",
        "display(predicted.limit(100))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the result, the **prediction** column contains the predicted value for the label, and the **trueLabel** column contains the actual known value from the testing data.\r\n",
        "\r\n",
        "Note that the **features** and **probability** columns are vectors. You can expand them to see the individual values they contain. The **features** vector contains the input variables for the model's algorithm.\r\n",
        "\r\n",
        "The **probability** vector contains the probability that the algorithm calculated for each possible class: 0 and 1 (which correspond to *not-late* and *late*). It looks like there are a mix of correct and incorrect predictions, and the ones that are incorrect tend to have fairly close probabilities for each class.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute confusion matrix metrics\r\n",
        "\r\n",
        "While we could explore the set of prediced and actual label values, and calculate accuracy by comparing how many predictions were correct compared to how many were wrong; which would be a tedious way to evaluate the mode. Additionally, a simple accuracy measure calculated this way may not be as useful an indication of model performance as you moight think. Consider that most flights, let's say 90%, arrive on-time. You could train a model that always predicts ***0***, and it would have an accuracy of 90% - it just wouln't be very helpful to predict which flights are likely to be a late!\r\n",
        "\r\n",
        "For these reasons, the predictive performamce of a binary classifier such as ours is usually measured using some standard metrics based on the *confusion matrix* formed by the number of cases in each of the following four categories:\r\n",
        "\r\n",
        "* *True Positives*: The predicted label and the actual label are both 1.\r\n",
        "* *False Positives*: The predicted label is 1, but the actual label is 0.\r\n",
        "* *False Negatives*: The predicted label is 0, but the actual label is 1.\r\n",
        "* *True Negatives*: The predicted label and the actual label are both 0.\r\n",
        "\r\n",
        "These metrics are generally tabulated for the test set and shown together as a *confusion matrix*, which takes the following form:\r\n",
        "\r\n",
        "<table style=\"border: 1px solid black;\">\r\n",
        "    <tr style=\"border: 1px solid black;\">\r\n",
        "        <td style=\"border: 1px solid black;color: black;\" bgcolor=\"lightgray\">TN</td><td style=\"border: 1px solid black;color: black;\" bgcolor=\"white\">FP</td>\r\n",
        "    </tr>\r\n",
        "    <tr style=\"border: 1px solid black;\">\r\n",
        "        <td style=\"border: 1px solid black;color: black;\" bgcolor=\"white\">FN</td><td style=\"border: 1px solid black;color: black;\" bgcolor=\"lightgray\">TP</td>\r\n",
        "    </tr>\r\n",
        "</table>\r\n",
        "\r\n",
        "From these metrics, some further evaluation measurements can be derived for each class:\r\n",
        "\r\n",
        "* *Precision*: Of the predictons the model made for this class, what proportion were correct?\r\n",
        "* *Recall*: Out of all of the instances of this class in the test dataset, how many did the model identify?\r\n",
        "* *F1-Score*: An average metric that takes both precision and recall for the given class into account.\r\n",
        "\r\n",
        "You can then average these metrics out across all possible classes to determine overall metrics."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.evaluation import MulticlassMetrics\r\n",
        "\r\n",
        "label_and_prediction = prediction.select([col('Late').cast(\"Double\").alias(\"label\"), col('prediction').cast(\"Double\")]) \\\r\n",
        "                            .rdd.map(lambda line: (line[1], line[0]))\r\n",
        "metrics = MulticlassMetrics(label_and_prediction)\r\n",
        "\r\n",
        "# Confusion Matrix\r\n",
        "cm = metrics.confusionMatrix().toArray()\r\n",
        "print(\"\\nConfusion Matrix:\")\r\n",
        "print(cm)\r\n",
        "print (\"\\nMetrics for class 0:\")\r\n",
        "print(\"Precision: %s\" % metrics.precision(0))\r\n",
        "print(\"Recall: %s\" % metrics.recall(0))\r\n",
        "print(\"F1: %s\" % metrics.fMeasure(0.0))\r\n",
        "print (\"\\nMetrics for class 1:\")\r\n",
        "print(\"Precision: %s\" % metrics.precision(1))\r\n",
        "print(\"Recall: %s\" % metrics.recall(1))\r\n",
        "print(\"F1: %s\" % metrics.fMeasure(1.0))\r\n",
        "print (\"\\nOverall metrics:\")\r\n",
        "print(\"Overall precision: %s\" % metrics.weightedPrecision)\r\n",
        "print(\"Overall recall: %s\" % metrics.weightedRecall)\r\n",
        "print(\"Overall F1: %s\" % metrics.weightedFMeasure())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The precision, recall, and F1 metrics are calculated as values between 0 and 1. The closer to 1 they are, the better the model is performing.\r\n",
        "\r\n",
        "Note also that the confusion matrix shows the intersections between counts of predicted and actual values. This means that a model that is predicting well will have a diagonal trend of higher values for the intersection of cases where the *actual* and *predicted* values are the same. This can often be easier to visualize as a heat map:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "classes = ['0 (on-time)', '1 (late)']\r\n",
        "plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\r\n",
        "plt.colorbar()\r\n",
        "tick_marks = np.arange(len(classes))\r\n",
        "plt.xticks(tick_marks, classes, rotation=45)\r\n",
        "plt.yticks(tick_marks, classes)\r\n",
        "plt.xlabel(\"Predicted Class\")\r\n",
        "plt.ylabel(\"Actual Class\")\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### View the raw prediction and probability\r\n",
        "\r\n",
        "The prediction is based on a raw prediction score that describes a labelled point in a logistic function. This raw prediction is then converted to a predicted label of 0 or 1 based on a probability vector that indicates the confidence for each possible label value (in this case, 0 and 1). The value with the highest confidence is selected as the prediction."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_predictions = prediction.select(\"rawPrediction\", \"probability\", col(\"prediction\").cast(\"int\"), col(\"Late\").alias(\"trueLabel\"))\r\n",
        "display(raw_predictions.limit(20))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the results include rows where the probability for 0 (the first value in the **probability** vector) is only slightly higher than the probability for 1 (the second value in the **probability** vector). The default *discrimination threshold* (the boundary that decides whether a probability is predicted as a 1 or a 0) is set to 0.5; so the prediction with the highest probability is always used, no matter how close to the threshold.\r\n",
        "\r\n",
        "### Review the Area Under ROC\r\n",
        "\r\n",
        "Moving the probability threshold above which the model predicts ***1*** would alter the number of true positives and false positives in the confusion matrix. A threshold of zero would predict every case as 1 (positive), while a threshold of 1 would predict every case as 0 (negative). The rate of true-postive and false-postive predictions between these thresholds is a good way to measure the predictive accuracy of a model. An ideal model will show a plotted line that rises vertically up the *true-positive rate* axis and stays at the maximum value (1) across the *false-positive rate*. The *area under the curve* (AUC) can then be calculated as a value between 0 and 1, with a value of 1 indicating a perfect model. In the case of a binary classification model, you can compare this AUC value with 0.5; which would represent the expected prediction rate for a random guess, and which can be plotted as a diagonal line through the ROC curve.\r\n",
        "\r\n",
        "It's generally easier to understand the AUC metric visually by plotting the ROC curve:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\r\n",
        "\r\n",
        "# Plot the ROC curve\r\n",
        "modelSummary = model.stages[-1].summary\r\n",
        "plt.plot([0, 1], [0, 1], 'r--')\r\n",
        "plt.plot(modelSummary.roc.select('FPR').collect(),\r\n",
        "         modelSummary.roc.select('TPR').collect())\r\n",
        "plt.xlabel('False Positive Rate')\r\n",
        "plt.ylabel('True Positive Rate')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# Get the AUC\r\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"Late\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\r\n",
        "auc = evaluator.evaluate(prediction)\r\n",
        "print (\"AUC = \", auc)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save and use a model for inferencing\r\n",
        "\r\n",
        "When you're happy with the performance of your model, you can save it. Then, when you need to use it for *inferencing* (predicting) labels for new data, you can load it and transform the new data with it."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import PipelineModel\r\n",
        "\r\n",
        "# Save the model\r\n",
        "saved_model_path = '/models/flights.model'\r\n",
        "model.write().overwrite().save(saved_model_path)\r\n",
        "\r\n",
        "# Load new data for flights in-progress (for which we have no Late\" label)\r\n",
        "flightSchema = StructType([\r\n",
        "  StructField(\"DayofMonth\", IntegerType(), False),\r\n",
        "  StructField(\"DayOfWeek\", IntegerType(), False),\r\n",
        "  StructField(\"Carrier\", StringType(), False),\r\n",
        "  StructField(\"OriginAirportID\", IntegerType(), False),\r\n",
        "  StructField(\"DestAirportID\", IntegerType(), False),\r\n",
        "  StructField(\"DepDelay\", IntegerType(), False)\r\n",
        "])\r\n",
        "\r\n",
        "new_data = spark.read.csv('/data/new-flights.csv', schema=flightSchema, header=True)\r\n",
        "\r\n",
        "# Load the model\r\n",
        "loaded_model = PipelineModel.load(saved_model_path)\r\n",
        "\r\n",
        "# Predict labels for the new data\r\n",
        "inferences = loaded_model.transform(new_data)\r\n",
        "\r\n",
        "display(inferences)\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, you've explored how to use Spark MLlib to train and use a machine learning model.\r\n",
        "\r\n",
        "We've only scratched the surface of machine learning with MLlib. To learn more, see the [Spark MLlib documentation](https://spark.apache.org/docs/latest/ml-guide.html)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}