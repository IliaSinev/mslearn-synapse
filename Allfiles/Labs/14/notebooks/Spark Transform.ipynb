{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transform Data with Spark in Azure Synapse Analytics\n",
        "\n",
        "Spark allows you to work with and manipulate data into information\n",
        "\n",
        "In this notebook, you'll consolidate heterogenous sources and create a homogenous output for consumption into a partitioned parquet format. This file can be used for consumption by a data scientist or data analyst for further analysis.\n",
        "\n",
        "> **Note**: This notebook is designed to be run in an Azure Synapse Analytics Spark pool.\n",
        "\n",
        "\n",
        "## Attach this notebook to a Spark pool\n",
        "\n",
        "To run the code in this notebook, you'll need to use a Spark pool; so at the top of this notebook, in the **Attach to** list, select your Spark pool that was created in the setup.ps1 script."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Explore Data\n",
        "\n",
        "Before training a model, a data engineer will explore the data to ensure that its profile matches what is expected, which is usually in the form of a technical specification. The use of notebooks though, allows data professionals to place these specifications within the notebook itself and allows for much greater collaboration throughout the organization.\n",
        "\n",
        "In this example, you'll explore some historical flight data with which we'll later transform into a denormalized structure and store it on disk for consumption by other data professionals downstream.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Data Using an Explicit Schema\n",
        "\n",
        "Let's start by loading some historical Sales Order data into a dataframe. If the structure of the data is known ahead of time, you can explicitly specify the schema for the dataframe.\n",
        "\n",
        "Review the code in the cell below, which defines a schema for Sales Order data before loading it from all of the csv files within the data directoyr. Then click the **&#9655;** button to the left of the cell to run it.\n",
        "\n",
        "> **Note**: The first time you run a cell in a notebook, the Spark pool must be started; which can take several minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "OrderSchema = StructType([\n",
        "  StructField(\"SalesOrderNumber\", StringType(), False),\n",
        "  StructField(\"SalesOrderLineNumber\", IntegerType(), False),\n",
        "  StructField(\"OrderDate\", DateType(), False),\n",
        "  StructField(\"CustomerName\", StringType(), False),\n",
        "  StructField(\"EmailAddress\", StringType(), False),\n",
        "  StructField(\"Item\", StringType(), False),\n",
        "  StructField(\"Quantity\", IntegerType(), False),\n",
        "  StructField(\"UnitPrice\", StringType(), False),\n",
        "  StructField(\"TaxAmount\", StringType(), False)\n",
        "])\n",
        "\n",
        "#let's populate the dataset using the schema noting that we're using the wildcard character \n",
        "#to grab all three of the csv files in the 'data' folder.\n",
        "OrderDetails = spark.read.csv('/data/*.csv', schema=OrderSchema, header=True)\n",
        "print(\"Data and schema loaded into OrderDetails successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "display(OrderDetails.limit(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Infer a Data Schema\n",
        "If the structure of the data source is unknown, you can have Spark automatically infer the schema.\n",
        "\n",
        "In this case, you will load data about *Orders* without knowing the schema.\n",
        "\n",
        "Run the following cell to load airport data from a text file, inferring the column names and data types automatically.\n",
        "\n",
        "> **Note**: You will want to ensure that schemas are the same when using a wildcard character to prevent troubleshooting errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "OrderDetails2019 = spark.read.csv('/data/*.csv', header=True, inferSchema=True)\n",
        "display(OrderDetails2019.limit(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Count the Rows in a Dataframes\n",
        "Now that you're familiar with working with dataframes, a key task when building predictive solutions is to explore the data, determing statistics that will help you understand the data before building predictive models. For example, how many rows of flight data do you actually have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This spans across all the files to perform a count within the DataFrame\n",
        "OrderDetails.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Count inferred rows\n",
        "This will execute the same query on inferred data from all of the files with a csv extension in the noted folder. The count should be the same as that above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# This code spans the single file pulled into the DataFrame\n",
        "OrderDetails2019.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Using a where clause to filter data\n",
        "The where clause can be applied to dataframes in PySpark and are an effective method to separate data into sub-groups, such as Order data as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# The first filter is by email address and performs a count.\n",
        "print('Total rows in DataFrame where \\\n",
        "EmailAddress = morgan30@adventure-works.com with where clause')\n",
        "print(OrderDetails.where(OrderDetails.EmailAddress == 'morgan30@adventure-works.com').count())\n",
        "  \n",
        "# The second uses the .show() function to list the results of the filtered dataframe query\n",
        "print('They are  ')\n",
        "OrderDetails.where(OrderDetails.EmailAddress == 'morgan30@adventure-works.com').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the same approach with a different column. \n",
        "You can view the show() function and other DataFrame functions at the [Apache Spark API Reference website](https://spark.apache.org/docs/3.2.0/api/python/reference/api/pyspark.sql.DataFrame.show.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print('Total rows in DataFrame where SalesOrderNumber = SO43705 with where clause')\n",
        "print(OrderDetails.where(OrderDetails.SalesOrderNumber == 'SO43705').count())\n",
        "  \n",
        "print('They are  ')\n",
        "OrderDetails.where(OrderDetails.SalesOrderNumber == 'SO45347').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use Dataframe Methods\n",
        "Spark DataFrames provide functions that you can use to extract and manipulate data. For example, you can use the **select** function to return a new dataframe containing columns selected from an existing dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# the use of DataFrames from another DataFrames is a quick way to create working tables\n",
        "# you will want to manage these as you go along but they allow the data engineer to try different analysis methods easily.\n",
        "OrderDetailSQL = OrderDetails.select(\"CustomerName\", \"OrderDate\", \"SalesOrderNumber\", \"OrderDate\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\")\n",
        "display(OrderDetailSQL.limit(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a Calendar table in the \n",
        "The next few steps will use the ***explode, sequence, and to_date functions*** to create a structure for populating a dim_calendar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import explode, sequence, to_date\n",
        "#set parameters of the dates to use\n",
        "startDate = '2010-01-01'\n",
        "endDate = '2025-12-31'\n",
        "\n",
        "(\n",
        "  spark.sql(f\"select explode(sequence(to_date('{startDate}'), to_date('{endDate}'), interval 1 day)) as calendarDate\")\n",
        "    .createOrReplaceTempView('calendar')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Expand on calendar view\n",
        "Let's add some more insight into the calendar table for encoding the Orders data further down to include Fiscal Year and Quarter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(f\"\"\"select year(calendarDate) * 10000 + month(calendarDate) * 100 + day(calendarDate) as dateid,\n",
        "  CalendarDate,\n",
        "  year(calendarDate) AS CalendarYear,\n",
        "  date_format(calendarDate, 'MMMM') as CalendarMonth,\n",
        "  month(calendarDate) as MonthOfYear,\n",
        "  date_format(calendarDate, 'EEEE') as CalendarDay,\n",
        "  dayofweek(calendarDate) AS DayOfWeek,\n",
        "  weekday(calendarDate) + 1 as DayOfWeekStartMonday,\n",
        "  case\n",
        "    when weekday(calendarDate) < 5 then 'Y'\n",
        "    else 'N'\n",
        "  end as IsWeekDay,\n",
        "  dayofmonth(calendarDate) as DayOfMonth,\n",
        "  case\n",
        "    when calendarDate = last_day(calendarDate) then 'Y'\n",
        "    else 'N'\n",
        "  end as IsLastDayOfMonth,\n",
        "  dayofyear(calendarDate) as DayOfYear,\n",
        "  weekofyear(calendarDate) as WeekOfYearIso,\n",
        "  quarter(calendarDate) as QuarterOfYear,\n",
        "  /* Use fiscal periods needed by organization fiscal calendar */\n",
        "  case\n",
        "    when month(calendarDate) >= 7 then year(calendarDate) + 1\n",
        "    else year(calendarDate)\n",
        "  end as FiscalYear,\n",
        "  (month(calendarDate) + 5) % 12 + 1 AS FiscalMonth\n",
        "from\n",
        "  calendar\n",
        "order by\n",
        "  calendarDate\"\"\").createOrReplaceTempView('dim_Calendar')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Query the table with a SELECT statement using magic key to first define the language. \n",
        "The current languages supported in Synapse Spark notebooks include: \n",
        "- PySpark (Python)\n",
        "- Spark (Scala)\n",
        "- Spark SQL\n",
        "- .NET Spark (C#)\n",
        "- SparkR (R) - In Preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "%%sql\n",
        "SELECT * from dim_Calendar LIMIT 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Split Customer Name into more searchable format\n",
        "Splitting the customer first name and last name is a common need in data transformation. It allows for easier searching. The following code will bring the OrderDetails dataframe as previously defined and add teh columns FirstName and LastName to the end which were split from teh CustomerName column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#Using the code above, let's create a new sql view/table\n",
        "### String Split of the column in pyspark\n",
        "from pyspark.sql.functions import split\n",
        " \n",
        "temp_df = OrderDetails.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n",
        "temp_df.createOrReplaceTempView(\"SQLOrderDetails\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Enocde the Calendar Information ###\n",
        "Now we can join the two temporary views together which split the customer name into two columns and join to the **dim_calendar** table \n",
        "to encode the data with a ***smart*** key and also brings in ***FiscalYear*** and ***FiscalMonth*** into the result set. We also create \n",
        "a new column named **OrderTotal** to be stored on disk with the other data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Create the temporary dataframe using a spark.sql query which joins the two temporary tables and returns transformed and encoded data.\n",
        "temp_df = spark.sql(f\"\"\"SELECT SalesOrderNumber, SalesOrderLineNumber, b.CalendarDate, b.FiscalYear, b.FiscalMonth, LastName as CustomerLastName, FirstName as CustomerFirstName, EmailAddress Item, Quantity, UnitPrice, TaxAmount, UnitPrice * Quantity + TaxAmount as OrderTotal FROM SQLOrderDetails a INNER JOIN dim_Calendar b\n",
        "ON a.OrderDate=b.CalendarDate ORDER BY OrderDate, CustomerName\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Partition the data by OrderDate and CustomerName\n",
        "The following code will create a set of files that are partitioned by OrderDate and CustomerName and store it in a parquet file format which is stored in a distributed fashion for higher compression of the files and for performance when working with the data in a distributed file system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {},
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "#write the partitioned dataframe\n",
        "temp_df.write.partitionBy(\"FiscalYear\",\"FiscalMonth\").mode(\"overwrite\").parquet(\"/data/OrdersTransform\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Query the newly created parquet file that is now in our Azure Data Lake\n",
        "we can now pull in the partitioned parquet file for consumption by other teams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# read content of file\n",
        "df = spark.read.parquet('/data/OrdersTransform')\n",
        "df.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Filter based on the partitioned columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "In this notebook, you've explored how to use a spark notebook to query data files within the datalake to perform some basic analysis with pyspark and pysql. You then exported those results into a format named parquet which is optimized for distributed and massively parrallel processsing (MPP) systems which are Analytical workloads. The value-add of parquet is that it is optimized for these workloads by its column-oriented structure and compression which can improve performance and reduce overall storage and compute costs.\n",
        "\n",
        "We've only scratched the surface of the power of notebooks. To learn more, see the [Apache Spark Notebooks Documentation](https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-development-using-notebooks)."
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {
        "842bc93c-1286-4aa2-8220-215018e6e39f": {
          "persist_state": {
            "view": {
              "chartOptions": {
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "0"
                ],
                "chartType": "bar",
                "isStacked": false,
                "seriesFieldKeys": [
                  "1"
                ]
              },
              "tableOptions": {},
              "type": "details"
            }
          },
          "sync_state": {
            "isSummary": false,
            "language": "scala",
            "table": {
              "rows": [
                {
                  "0": "SO43701",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Christy Zhu",
                  "4": "christy12@adventure-works.com",
                  "5": "Mountain-100 Silver, 44",
                  "6": "1",
                  "7": "3399.99",
                  "8": "271.9992"
                },
                {
                  "0": "SO43704",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Julio Ruiz",
                  "4": "julio1@adventure-works.com",
                  "5": "Mountain-100 Black, 48",
                  "6": "1",
                  "7": "3374.99",
                  "8": "269.9992"
                },
                {
                  "0": "SO43705",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Curtis Lu",
                  "4": "curtis9@adventure-works.com",
                  "5": "Mountain-100 Silver, 38",
                  "6": "1",
                  "7": "3399.99",
                  "8": "271.9992"
                },
                {
                  "0": "SO43700",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Ruben Prasad",
                  "4": "ruben10@adventure-works.com",
                  "5": "Road-650 Black, 62",
                  "6": "1",
                  "7": "699.0982",
                  "8": "55.9279"
                },
                {
                  "0": "SO43703",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Albert Alvarez",
                  "4": "albert7@adventure-works.com",
                  "5": "Road-150 Red, 62",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43697",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Cole Watson",
                  "4": "cole1@adventure-works.com",
                  "5": "Road-150 Red, 62",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43699",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Sydney Wright",
                  "4": "sydney61@adventure-works.com",
                  "5": "Mountain-100 Silver, 44",
                  "6": "1",
                  "7": "3399.99",
                  "8": "271.9992"
                },
                {
                  "0": "SO43702",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Colin Anand",
                  "4": "colin45@adventure-works.com",
                  "5": "Road-150 Red, 44",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43698",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Rachael Martinez",
                  "4": "rachael16@adventure-works.com",
                  "5": "Mountain-100 Silver, 44",
                  "6": "1",
                  "7": "3399.99",
                  "8": "271.9992"
                },
                {
                  "0": "SO43707",
                  "1": "1",
                  "2": "2019-07-02",
                  "3": "Emma Brown",
                  "4": "emma3@adventure-works.com",
                  "5": "Road-150 Red, 48",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43711",
                  "1": "1",
                  "2": "2019-07-02",
                  "3": "Courtney Edwards",
                  "4": "courtney1@adventure-works.com",
                  "5": "Road-150 Red, 56",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43706",
                  "1": "1",
                  "2": "2019-07-02",
                  "3": "Edward Brown",
                  "4": "edward26@adventure-works.com",
                  "5": "Road-150 Red, 48",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43708",
                  "1": "1",
                  "2": "2019-07-02",
                  "3": "Brad Deng",
                  "4": "brad2@adventure-works.com",
                  "5": "Road-650 Red, 52",
                  "6": "1",
                  "7": "699.0982",
                  "8": "55.9279"
                },
                {
                  "0": "SO43709",
                  "1": "1",
                  "2": "2019-07-02",
                  "3": "Martha Xu",
                  "4": "martha12@adventure-works.com",
                  "5": "Road-150 Red, 52",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43710",
                  "1": "1",
                  "2": "2019-07-02",
                  "3": "Katrina Raji",
                  "4": "katrina20@adventure-works.com",
                  "5": "Road-150 Red, 56",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43712",
                  "1": "1",
                  "2": "2019-07-02",
                  "3": "Abigail Henderson",
                  "4": "abigail73@adventure-works.com",
                  "5": "Road-150 Red, 44",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43720",
                  "1": "1",
                  "2": "2019-07-03",
                  "3": "Melanie Sanchez",
                  "4": "melanie47@adventure-works.com",
                  "5": "Road-150 Red, 44",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43721",
                  "1": "1",
                  "2": "2019-07-03",
                  "3": "Louis Xie",
                  "4": "louis20@adventure-works.com",
                  "5": "Road-150 Red, 62",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43714",
                  "1": "1",
                  "2": "2019-07-03",
                  "3": "Latasha Alonso",
                  "4": "latasha8@adventure-works.com",
                  "5": "Road-150 Red, 44",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43715",
                  "1": "1",
                  "2": "2019-07-03",
                  "3": "Warren Jai",
                  "4": "warren42@adventure-works.com",
                  "5": "Road-150 Red, 56",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "SalesOrderNumber",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "SalesOrderLineNumber",
                  "type": "int"
                },
                {
                  "key": "2",
                  "name": "OrderDate",
                  "type": "date"
                },
                {
                  "key": "3",
                  "name": "CustomerName",
                  "type": "string"
                },
                {
                  "key": "4",
                  "name": "EmailAddress",
                  "type": "string"
                },
                {
                  "key": "5",
                  "name": "Item",
                  "type": "string"
                },
                {
                  "key": "6",
                  "name": "Quantity",
                  "type": "int"
                },
                {
                  "key": "7",
                  "name": "UnitPrice",
                  "type": "string"
                },
                {
                  "key": "8",
                  "name": "TaxAmount",
                  "type": "string"
                }
              ],
              "truncated": false
            }
          },
          "type": "Synapse.DataFrame"
        }
      },
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
