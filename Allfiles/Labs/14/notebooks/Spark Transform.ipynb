{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transform Data with Spark in Azure Synapse Analytics\n",
        "\n",
        "Spark allows you to work with and manipulate data into information\n",
        "\n",
        "In this notebook, you'll consolidate heterogenous sources and create a homogenous output for consumption into a partitioned parquet format. This file can be used for consumption by a data scientist or data analyst for further analysis.\n",
        "\n",
        "> **Note**: This notebook is designed to be run in an Azure Synapse Analytics Spark pool.\n",
        "\n",
        "\n",
        "## Attach this notebook to a Spark pool\n",
        "\n",
        "To run the code in this notebook, you'll need to use a Spark pool; so at the top of this notebook, in the **Attach to** list, select your Spark pool that was created in the setup.ps1 script."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Explore Data\n",
        "\n",
        "Before training a model, a data engineer will explore the data to ensure that its profile matches what is expected, which is usually in the form of a technical specification. The use of notebooks though, allows data professionals to place these specifications within the notebook itself and allows for much greater collaboration throughout the organization.\n",
        "\n",
        "In this example, you'll explore some historical flight data with which we'll later transform into a denormalized structure and store it on disk for consumption by other data professionals downstream.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Data Using an Explicit Schema\n",
        "\n",
        "Let's start by loading some historical Sales Order data into a dataframe. If the structure of the data is known ahead of time, you can explicitly specify the schema for the dataframe.\n",
        "\n",
        "Review the code in the cell below, which defines a schema for Sales Order data before loading it from all of the csv files within the data directoyr. Then click the **&#9655;** button to the left of the cell to run it.\n",
        "\n",
        "> **Note**: The first time you run a cell in a notebook, the Spark pool must be started; which can take several minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "OrderSchema = StructType([\n",
        "  StructField(\"SalesOrderNumber\", StringType(), False),\n",
        "  StructField(\"SalesOrderLineNumber\", IntegerType(), False),\n",
        "  StructField(\"OrderDate\", DateType(), False),\n",
        "  StructField(\"CustomerName\", StringType(), False),\n",
        "  StructField(\"EmailAddress\", StringType(), False),\n",
        "  StructField(\"Item\", StringType(), False),\n",
        "  StructField(\"Quantity\", IntegerType(), False),\n",
        "  StructField(\"UnitPrice\", StringType(), False),\n",
        "  StructField(\"TaxAmount\", StringType(), False)\n",
        "])\n",
        "\n",
        "#let's populate the dataset using the schema noting that we're using the wildcard character \n",
        "#to grab all three of the csv files in the 'data' folder.\n",
        "OrderDetails = spark.read.csv('/data/*.csv', schema=OrderSchema, header=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "display(OrderDetails.limit(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Infer a Data Schema\n",
        "If the structure of the data source is unknown, you can have Spark automatically infer the schema.\n",
        "\n",
        "In this case, you will load data about *Orders* without knowing the schema.\n",
        "\n",
        "Run the following cell to load airport data from a text file, inferring the column names and data types automatically.\n",
        "\n",
        "> **Note**: You will want to ensure that schemas are the same when using a wildcard character to prevent troubleshooting errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "OrderDetails2019 = spark.read.csv('/data/*.csv', header=True, inferSchema=True)\n",
        "#display(OrderDetails2019.limit(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Count the Rows in a Dataframes\n",
        "Now that you're familiar with working with dataframes, a key task when building predictive solutions is to explore the data, determing statistics that will help you understand the data before building predictive models. For example, how many rows of flight data do you actually have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This spans across all the files to perform a count within the DataFrame\n",
        "OrderDetails.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Count inferred rows\n",
        "This will execute the same query on inferred data from all of the files with a csv extension in the noted folder. The count should be the same as that above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# This code spans the single file pulled into the DataFrame\n",
        "OrderDetails2019.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Using a where clause to filter data\n",
        "The where clause can be applied to dataframes in PySpark and are an effective method to separate data into sub-groups, such as Order data as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# The first filter is by email address and performs a count.\n",
        "print('Total rows in DataFrame where \\\n",
        "EmailAddress = morgan30@adventure-works.com with where clause')\n",
        "print(OrderDetails.where(OrderDetails.EmailAddress == 'morgan30@adventure-works.com').count())\n",
        "  \n",
        "# The second uses the .show() function to list the results of the filtered dataframe query\n",
        "print('They are  ')\n",
        "OrderDetails.where(OrderDetails.EmailAddress == 'morgan30@adventure-works.com').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the same approach with a different column. \n",
        "You can view the show() function and other DataFrame functions at the [Apache Spark API Reference website](https://spark.apache.org/docs/3.2.0/api/python/reference/api/pyspark.sql.DataFrame.show.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print('Total rows in DataFrame where SalesOrderNumber = SO43705 with where clause')\n",
        "print(OrderDetails.where(OrderDetails.SalesOrderNumber == 'SO43705').count())\n",
        "  \n",
        "print('They are  ')\n",
        "OrderDetails.where(OrderDetails.SalesOrderNumber == 'SO45347').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use Dataframe Methods\n",
        "Spark DataFrames provide functions that you can use to extract and manipulate data. For example, you can use the **select** function to return a new dataframe containing columns selected from an existing dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# the use of DataFrames from another DataFrames is a quick way to create working tables\n",
        "# you will want to manage these as you go along but they allow the data engineer to try different analysis methods easily.\n",
        "OrderDetailSQL = OrderDetails.select(\"CustomerName\", \"OrderDate\", \"SalesOrderNumber\", \"OrderDate\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\")\n",
        "display(OrderDetailSQL.limit(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extract Data into a Lookup table and add a surrogate key\n",
        "Often when we receive denormalized data which can "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Split Customer Name into more searchable format\n",
        "Splitting the customer first name and last name is a common need in data transformation. It allows for easier searching. The following code will bring the OrderDetails dataframe as previously defined and add teh columns FirstName and LastName to the end which were split from teh CustomerName column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "### String Split of the column in pyspark\n",
        "from pyspark.sql.functions import split\n",
        " \n",
        "OrderDetails.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1)).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### SparkSQL\n",
        "Now, let's take a look at how we can query using a language more familiar to some data engineers. starting by creating a view or table from a spark dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#Using the code above, let's create a new sql view/table\n",
        "### String Split of the column in pyspark\n",
        "from pyspark.sql.functions import split\n",
        " \n",
        "temp_df = OrderDetails.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n",
        "temp_df.createOrReplaceTempView(\"SQLOrderDetails\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Create the view from the temporary DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "temp_df.createOrReplaceTempView(\"SQLOrderDetails\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Using the ***magic*** keys to switch to a different language ####\n",
        "\n",
        "The ***magic*** key allows you to switch from any of the supported languages within the same notebook which allows for great collaboration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "SELECT LastName, FirstName, count(SalesOrderNumber) FROM SQLOrderDetails GROUP BY  LastName, FirstName HAVING count(SalesOrderNumber) > 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### The results of this or any other query can be viewed as a table or a chart allowing quick analysis by the data engineer on the data shape ####\n",
        "View the output as a chart, and set the view options as follows:\n",
        "\n",
        "- **Chart type**: Line chart\n",
        "- **Key**: LastName\n",
        "- **Values**: count(SalesOrderNumber)\n",
        "- **Series Group**: *blank*\n",
        "- **Aggregation**: Sum\n",
        "\n",
        "The line chart shows the number of orders broken down by customer last name. The data scientist can further enhance this data and look for correlation using different analysis techniques. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Show Duplicate Rows from the dataset, if any\n",
        "### flag or check Duplicate rows in pyspark\n",
        " \n",
        "import pyspark.sql.functions as f\n",
        "temp_df.join(\n",
        "    temp_df.groupBy(df_basket1.columns).agg((f.count(\"*\")>1).cast(\"int\").alias(\"Duplicate_indicator\")),\n",
        "    on=df_basket1.columns,\n",
        "    how=\"inner\"\n",
        ").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Partition the data by OrderDate and CustomerName\n",
        "The following code will create a set of files that are partitioned by OrderDate and CustomerName and store it in a parquet file format which is stored in a distributed fashion for higher compression of the files and for performance when working with the data in a distributed file system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {},
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "temp_df.write.partitionBy('OrderDate', 'CustomerName').parquet('OrderDetailsExpanded')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "In this notebook, you've explored how to use a spark notebook to query data files within the datalake to perform some basic analysis with pyspark and pysql. You then exported those results into a format named parquet which is optimized for distributed and massively parrallel processsing (MPP) systems.\n",
        "\n",
        "We've only scratched the surface of the power of notebooks. To learn more, see the [Apache Spark Notebooks Documentation](https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-development-using-notebooks)."
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {
        "842bc93c-1286-4aa2-8220-215018e6e39f": {
          "persist_state": {
            "view": {
              "chartOptions": {
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "0"
                ],
                "chartType": "bar",
                "isStacked": false,
                "seriesFieldKeys": [
                  "1"
                ]
              },
              "tableOptions": {},
              "type": "details"
            }
          },
          "sync_state": {
            "isSummary": false,
            "language": "scala",
            "table": {
              "rows": [
                {
                  "0": "SO43701",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Christy Zhu",
                  "4": "christy12@adventure-works.com",
                  "5": "Mountain-100 Silver, 44",
                  "6": "1",
                  "7": "3399.99",
                  "8": "271.9992"
                },
                {
                  "0": "SO43704",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Julio Ruiz",
                  "4": "julio1@adventure-works.com",
                  "5": "Mountain-100 Black, 48",
                  "6": "1",
                  "7": "3374.99",
                  "8": "269.9992"
                },
                {
                  "0": "SO43705",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Curtis Lu",
                  "4": "curtis9@adventure-works.com",
                  "5": "Mountain-100 Silver, 38",
                  "6": "1",
                  "7": "3399.99",
                  "8": "271.9992"
                },
                {
                  "0": "SO43700",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Ruben Prasad",
                  "4": "ruben10@adventure-works.com",
                  "5": "Road-650 Black, 62",
                  "6": "1",
                  "7": "699.0982",
                  "8": "55.9279"
                },
                {
                  "0": "SO43703",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Albert Alvarez",
                  "4": "albert7@adventure-works.com",
                  "5": "Road-150 Red, 62",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43697",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Cole Watson",
                  "4": "cole1@adventure-works.com",
                  "5": "Road-150 Red, 62",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43699",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Sydney Wright",
                  "4": "sydney61@adventure-works.com",
                  "5": "Mountain-100 Silver, 44",
                  "6": "1",
                  "7": "3399.99",
                  "8": "271.9992"
                },
                {
                  "0": "SO43702",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Colin Anand",
                  "4": "colin45@adventure-works.com",
                  "5": "Road-150 Red, 44",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43698",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Rachael Martinez",
                  "4": "rachael16@adventure-works.com",
                  "5": "Mountain-100 Silver, 44",
                  "6": "1",
                  "7": "3399.99",
                  "8": "271.9992"
                },
                {
                  "0": "SO43707",
                  "1": "1",
                  "2": "2019-07-02",
                  "3": "Emma Brown",
                  "4": "emma3@adventure-works.com",
                  "5": "Road-150 Red, 48",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43711",
                  "1": "1",
                  "2": "2019-07-02",
                  "3": "Courtney Edwards",
                  "4": "courtney1@adventure-works.com",
                  "5": "Road-150 Red, 56",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43706",
                  "1": "1",
                  "2": "2019-07-02",
                  "3": "Edward Brown",
                  "4": "edward26@adventure-works.com",
                  "5": "Road-150 Red, 48",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43708",
                  "1": "1",
                  "2": "2019-07-02",
                  "3": "Brad Deng",
                  "4": "brad2@adventure-works.com",
                  "5": "Road-650 Red, 52",
                  "6": "1",
                  "7": "699.0982",
                  "8": "55.9279"
                },
                {
                  "0": "SO43709",
                  "1": "1",
                  "2": "2019-07-02",
                  "3": "Martha Xu",
                  "4": "martha12@adventure-works.com",
                  "5": "Road-150 Red, 52",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43710",
                  "1": "1",
                  "2": "2019-07-02",
                  "3": "Katrina Raji",
                  "4": "katrina20@adventure-works.com",
                  "5": "Road-150 Red, 56",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43712",
                  "1": "1",
                  "2": "2019-07-02",
                  "3": "Abigail Henderson",
                  "4": "abigail73@adventure-works.com",
                  "5": "Road-150 Red, 44",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43720",
                  "1": "1",
                  "2": "2019-07-03",
                  "3": "Melanie Sanchez",
                  "4": "melanie47@adventure-works.com",
                  "5": "Road-150 Red, 44",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43721",
                  "1": "1",
                  "2": "2019-07-03",
                  "3": "Louis Xie",
                  "4": "louis20@adventure-works.com",
                  "5": "Road-150 Red, 62",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43714",
                  "1": "1",
                  "2": "2019-07-03",
                  "3": "Latasha Alonso",
                  "4": "latasha8@adventure-works.com",
                  "5": "Road-150 Red, 44",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43715",
                  "1": "1",
                  "2": "2019-07-03",
                  "3": "Warren Jai",
                  "4": "warren42@adventure-works.com",
                  "5": "Road-150 Red, 56",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "SalesOrderNumber",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "SalesOrderLineNumber",
                  "type": "int"
                },
                {
                  "key": "2",
                  "name": "OrderDate",
                  "type": "date"
                },
                {
                  "key": "3",
                  "name": "CustomerName",
                  "type": "string"
                },
                {
                  "key": "4",
                  "name": "EmailAddress",
                  "type": "string"
                },
                {
                  "key": "5",
                  "name": "Item",
                  "type": "string"
                },
                {
                  "key": "6",
                  "name": "Quantity",
                  "type": "int"
                },
                {
                  "key": "7",
                  "name": "UnitPrice",
                  "type": "string"
                },
                {
                  "key": "8",
                  "name": "TaxAmount",
                  "type": "string"
                }
              ],
              "truncated": false
            }
          },
          "type": "Synapse.DataFrame"
        }
      },
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
