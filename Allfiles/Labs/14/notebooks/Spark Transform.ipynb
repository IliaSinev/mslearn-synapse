{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transform Data with Spark in Azure Synapse Analytics\n",
        "\n",
        "Spark allows you to work with and manipulate data into information\n",
        "\n",
        "In this notebook, you'll consolidate heterogenous sources and create a homogenous output for consumption into a partitioned parquet format. This file can be used for consumption by a data scientist or data analyst for further analysis.\n",
        "\n",
        "> **Note**: This notebook is designed to be run in an Azure Synapse Analytics Spark pool.\n",
        "\n",
        "\n",
        "## Attach this notebook to a Spark pool\n",
        "\n",
        "To run the code in this notebook, you'll need to use a Spark pool; so at the top of this notebook, in the **Attach to** list, select your Spark pool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Explore Data\n",
        "\n",
        "Before training a model, a data engineer will explore the data to ensure that its profile matches what is expected, which is usually in the form of a technical specification. The use of notebooks though, allows data professionals to place these specifications within the notebook itself and allows for much greater collaboration throughout the organization.\n",
        "\n",
        "In this example, you'll explore some historical flight data with which we'll later transform into a denormalized structure and store it on disk for consumption by other data professionals downstream.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Data Using an Explicit Schema\n",
        "\n",
        "Let's start by loading some historical Sales Order data into a dataframe. If the structure of the data is known ahead of time, you can explicitly specify the schema for the dataframe.\n",
        "\n",
        "Review the code in the cell below, which defines a schema for Sales Order data before loading it from all of the csv files within the data directoyr. Then click the **&#9655;** button to the left of the cell to run it.\n",
        "\n",
        "> **Note**: The first time you run a cell in a notebook, the Spark pool must be started; which can take several minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "OrderSchema = StructType([\n",
        "  StructField(\"SalesOrderNumber\", StringType(), False),\n",
        "  StructField(\"SalesOrderLineNumber\", IntegerType(), False),\n",
        "  StructField(\"OrderDate\", DateType(), False),\n",
        "  StructField(\"CustomerName\", StringType(), False),\n",
        "  StructField(\"EmailAddress\", StringType(), False),\n",
        "  StructField(\"Item\", StringType(), False),\n",
        "  StructField(\"Quantity\", IntegerType(), False),\n",
        "  StructField(\"UnitPrice\", StringType(), False),\n",
        "  StructField(\"TaxAmount\", StringType(), False)\n",
        "])\n",
        "\n",
        "#let's not query the entire dataset, instead let's look at the top 20 results\n",
        "OrderDetails = spark.read.csv('/data/*.csv', schema=OrderSchema, header=True)\n",
        "display(OrderDetails.limit(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "display(OrderDetails.limit(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Infer a Data Schema\n",
        "If the structure of the data source is unknown, you can have Spark automatically infer the schema.\n",
        "\n",
        "In this case, you will load data about airports without knowing the schema.\n",
        "\n",
        "Run the following cell to load airport data from a text file, inferring the column names and data types automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "OrderDetails2019 = spark.read.csv('/data/*.csv', header=True, inferSchema=True)\n",
        "#display(OrderDetails2019.limit(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Count the Rows in a Dataframes\n",
        "Now that you're familiar with working with dataframes, a key task when building predictive solutions is to explore the data, determing statistics that will help you understand the data before building predictive models. For example, how many rows of flight data do you actually have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This spans across all the files to perform a count within the dataframe\n",
        "OrderDetails.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Count inferred rows\n",
        "This will execute the same query on inferred data from all of the files with a csv extension in the noted folder. The count should be the same as that above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# This code spans the single file pulled into the dataframe\n",
        "OrderDetails2019.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Remove any headers from the consolidation of the source files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "OrderDetails = OrderDetails.filter(OrderDetails.SalesOrderNumber == \"SalesOrderNumber\")\n",
        "print(OrderDetails)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print('Total rows in dataframe where \\\n",
        "EmailAddress = morgan30@adventure-works.com with where clause')\n",
        "print(OrderDetails.where(OrderDetails.EmailAddress == 'morgan30@adventure-works.com').count())\n",
        "  \n",
        "print('They are  ')\n",
        "OrderDetails.where(OrderDetails.EmailAddress == 'morgan30@adventure-works.com').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print('Total rows in dataframe where SalesOrderNumber = SO43705 with where clause')\n",
        "print(OrderDetails.where(OrderDetails.SalesOrderNumber == 'SO43705').count())\n",
        "  \n",
        "print('They are  ')\n",
        "OrderDetails.where(OrderDetails.SalesOrderNumber == 'SO45347').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use Dataframe Methods\n",
        "Spark DataFrames provide functions that you can use to extract and manipulate data. For example, you can use the **select** function to return a new dataframe containing columns selected from an existing dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# the use of dataframes from anoher dataframe is a quick way to create working tables\n",
        "# you will want to manage these as you go along but they allow the data engineer to try different analysis methods easily.\n",
        "OrderDetailSQL = OrderDetails.select(\"CustomerName\", \"OrderDate\", \"SalesOrderNumber\", \"OrderDate\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\")\n",
        "display(OrderDetailSQL.limit(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Split Customer Name into more searchable format\n",
        "Splitting the customer first name and last name is a common need in data transformation. It allows for easier searching. The following code will bring the OrderDetails dataframe as previously defined and add teh columns FirstName and LastName to the end which were split from teh CustomerName column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "### String Split of the column in pyspark\n",
        "from pyspark.sql.functions import split\n",
        " \n",
        "OrderDetails.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1)).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### SparkSQL\n",
        "Now, let's take a look at how we can query using a language more familiar to some data engineers. starting by creating a view or table from a spark dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#Using the code above, let's create a new sql view/table\n",
        "### String Split of the column in pyspark\n",
        "from pyspark.sql.functions import split\n",
        " \n",
        "temp_df = OrderDetails.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n",
        "temp_df.createOrReplaceTempView(\"SQLOrderDetails\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Create the view from the temporary dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "temp_df.createOrReplaceTempView(\"SQLOrderDetails\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "SELECT LastName, FirstName, count(SalesOrderNumber) FROM SQLOrderDetails GROUP BY  LastName, FirstName HAVING count(SalesOrderNumber) > 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "View the output as a chart, and set the view options as follows:\n",
        "\n",
        "- **Chart type**: Line chart\n",
        "- **Key**: LastName\n",
        "- **Values**: count(SalesOrderNumber)\n",
        "- **Series Group**: *blank*\n",
        "- **Aggregation**: Sum\n",
        "\n",
        "The line chart shows the number of orders broken down by customer last name. The data scientist can further enhance this data and look for correlation using different analysis techniques. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Show Duplicate Rows from the dataset, if any\n",
        "### flag or check Duplicate rows in pyspark\n",
        " \n",
        "import pyspark.sql.functions as f\n",
        "temp_df.join(\n",
        "    temp_df.groupBy(df_basket1.columns).agg((f.count(\"*\")>1).cast(\"int\").alias(\"Duplicate_indicator\")),\n",
        "    on=df_basket1.columns,\n",
        "    how=\"inner\"\n",
        ").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Partition the data by OrderDate and CustomerName\n",
        "The following code will create a set of files that are partitioned by OrderDate and CustomerName and store it in a parquet file format which is stored in a distributed fashion for higher compression of the files and for performance when working with the data in a distributed file system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {},
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "temp_df.write.partitionBy('OrderDate', 'CustomerName').parquet('OrderDetailsExpanded')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "In this notebook, you've explored how to use a spark notebook to query data files within the datalake to perform some basic analysis with pyspark and pysql. You then exported those results into a format named parquet which is optimized for distributed and massively parrallel processsing (MPP) systems.\n",
        "\n",
        "We've only scratched the surface of the power of notebooks. To learn more, see the [Apache Spark Notebooks Documentation](https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-development-using-notebooks)."
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {
        "842bc93c-1286-4aa2-8220-215018e6e39f": {
          "persist_state": {
            "view": {
              "chartOptions": {
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "0"
                ],
                "chartType": "bar",
                "isStacked": false,
                "seriesFieldKeys": [
                  "1"
                ]
              },
              "tableOptions": {},
              "type": "details"
            }
          },
          "sync_state": {
            "isSummary": false,
            "language": "scala",
            "table": {
              "rows": [
                {
                  "0": "SO43701",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Christy Zhu",
                  "4": "christy12@adventure-works.com",
                  "5": "Mountain-100 Silver, 44",
                  "6": "1",
                  "7": "3399.99",
                  "8": "271.9992"
                },
                {
                  "0": "SO43704",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Julio Ruiz",
                  "4": "julio1@adventure-works.com",
                  "5": "Mountain-100 Black, 48",
                  "6": "1",
                  "7": "3374.99",
                  "8": "269.9992"
                },
                {
                  "0": "SO43705",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Curtis Lu",
                  "4": "curtis9@adventure-works.com",
                  "5": "Mountain-100 Silver, 38",
                  "6": "1",
                  "7": "3399.99",
                  "8": "271.9992"
                },
                {
                  "0": "SO43700",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Ruben Prasad",
                  "4": "ruben10@adventure-works.com",
                  "5": "Road-650 Black, 62",
                  "6": "1",
                  "7": "699.0982",
                  "8": "55.9279"
                },
                {
                  "0": "SO43703",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Albert Alvarez",
                  "4": "albert7@adventure-works.com",
                  "5": "Road-150 Red, 62",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43697",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Cole Watson",
                  "4": "cole1@adventure-works.com",
                  "5": "Road-150 Red, 62",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43699",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Sydney Wright",
                  "4": "sydney61@adventure-works.com",
                  "5": "Mountain-100 Silver, 44",
                  "6": "1",
                  "7": "3399.99",
                  "8": "271.9992"
                },
                {
                  "0": "SO43702",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Colin Anand",
                  "4": "colin45@adventure-works.com",
                  "5": "Road-150 Red, 44",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43698",
                  "1": "1",
                  "2": "2019-07-01",
                  "3": "Rachael Martinez",
                  "4": "rachael16@adventure-works.com",
                  "5": "Mountain-100 Silver, 44",
                  "6": "1",
                  "7": "3399.99",
                  "8": "271.9992"
                },
                {
                  "0": "SO43707",
                  "1": "1",
                  "2": "2019-07-02",
                  "3": "Emma Brown",
                  "4": "emma3@adventure-works.com",
                  "5": "Road-150 Red, 48",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43711",
                  "1": "1",
                  "2": "2019-07-02",
                  "3": "Courtney Edwards",
                  "4": "courtney1@adventure-works.com",
                  "5": "Road-150 Red, 56",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43706",
                  "1": "1",
                  "2": "2019-07-02",
                  "3": "Edward Brown",
                  "4": "edward26@adventure-works.com",
                  "5": "Road-150 Red, 48",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43708",
                  "1": "1",
                  "2": "2019-07-02",
                  "3": "Brad Deng",
                  "4": "brad2@adventure-works.com",
                  "5": "Road-650 Red, 52",
                  "6": "1",
                  "7": "699.0982",
                  "8": "55.9279"
                },
                {
                  "0": "SO43709",
                  "1": "1",
                  "2": "2019-07-02",
                  "3": "Martha Xu",
                  "4": "martha12@adventure-works.com",
                  "5": "Road-150 Red, 52",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43710",
                  "1": "1",
                  "2": "2019-07-02",
                  "3": "Katrina Raji",
                  "4": "katrina20@adventure-works.com",
                  "5": "Road-150 Red, 56",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43712",
                  "1": "1",
                  "2": "2019-07-02",
                  "3": "Abigail Henderson",
                  "4": "abigail73@adventure-works.com",
                  "5": "Road-150 Red, 44",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43720",
                  "1": "1",
                  "2": "2019-07-03",
                  "3": "Melanie Sanchez",
                  "4": "melanie47@adventure-works.com",
                  "5": "Road-150 Red, 44",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43721",
                  "1": "1",
                  "2": "2019-07-03",
                  "3": "Louis Xie",
                  "4": "louis20@adventure-works.com",
                  "5": "Road-150 Red, 62",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43714",
                  "1": "1",
                  "2": "2019-07-03",
                  "3": "Latasha Alonso",
                  "4": "latasha8@adventure-works.com",
                  "5": "Road-150 Red, 44",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                },
                {
                  "0": "SO43715",
                  "1": "1",
                  "2": "2019-07-03",
                  "3": "Warren Jai",
                  "4": "warren42@adventure-works.com",
                  "5": "Road-150 Red, 56",
                  "6": "1",
                  "7": "3578.27",
                  "8": "286.2616"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "SalesOrderNumber",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "SalesOrderLineNumber",
                  "type": "int"
                },
                {
                  "key": "2",
                  "name": "OrderDate",
                  "type": "date"
                },
                {
                  "key": "3",
                  "name": "CustomerName",
                  "type": "string"
                },
                {
                  "key": "4",
                  "name": "EmailAddress",
                  "type": "string"
                },
                {
                  "key": "5",
                  "name": "Item",
                  "type": "string"
                },
                {
                  "key": "6",
                  "name": "Quantity",
                  "type": "int"
                },
                {
                  "key": "7",
                  "name": "UnitPrice",
                  "type": "string"
                },
                {
                  "key": "8",
                  "name": "TaxAmount",
                  "type": "string"
                }
              ],
              "truncated": false
            }
          },
          "type": "Synapse.DataFrame"
        }
      },
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
