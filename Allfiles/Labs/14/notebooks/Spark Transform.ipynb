{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transform Data with Spark in Azure Synapse Analytics\n",
        "\n",
        "Spark allows you to work with and manipulate data into information\n",
        "\n",
        "In this notebook, you'll consolidate heterogenous sources and create a homogenous output for consumption into a partitioned parquet format. This file can be used for consumption by a data scientist or data analyst for further analysis.\n",
        "\n",
        "> **Note**: This notebook is designed to be run in an Azure Synapse Analytics Spark pool.\n",
        "\n",
        "\n",
        "## Attach this notebook to a Spark pool\n",
        "\n",
        "To run the code in this notebook, you'll need to use a Spark pool; so at the top of this notebook, in the **Attach to** list, select your Spark pool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Explore Data\n",
        "\n",
        "Before training a model, a data engineer will explore the data to ensure that its profile matches what is expected, which is usually in the form of a technical specification. The use of notebooks though, allows data professionals to place these specifications within the notebook itself and allows for much greater collaboration throughout the organization.\n",
        "\n",
        "In this example, you'll explore some historical flight data with which we'll later transform into a denormalized structure and store it on disk for consumption by other data professionals downstream.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Data Using an Explicit Schema\n",
        "\n",
        "Let's start by loading some historical Sales Order data into a dataframe. If the structure of the data is known ahead of time, you can explicitly specify the schema for the dataframe.\n",
        "\n",
        "Review the code in the cell below, which defines a schema for Sales Order data before loading it from all of the csv files within the data directoyr. Then click the **&#9655;** button to the left of the cell to run it.\n",
        "\n",
        "> **Note**: The first time you run a cell in a notebook, the Spark pool must be started; which can take several minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-10-24T13:43:22.6437716Z",
              "execution_start_time": "2022-10-24T13:42:55.0687107Z",
              "livy_statement_state": "available",
              "queued_time": "2022-10-24T13:39:22.8999666Z",
              "session_id": "0",
              "session_start_time": "2022-10-24T13:39:22.9459005Z",
              "spark_jobs": null,
              "spark_pool": "sparkhc9o2ay",
              "state": "finished",
              "statement_id": 1
            },
            "text/plain": [
              "StatementMeta(sparkhc9o2ay, 0, 1, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "97b31a73-c658-4138-8298-9501f762a9de",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": [
              "SynapseWidget(Synapse.DataFrame, 97b31a73-c658-4138-8298-9501f762a9de)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "OrderSchema = StructType([\n",
        "  StructField(\"SalesOrderNumber\", StringType(), False),\n",
        "  StructField(\"SalesOrderLineNumber\", IntegerType(), False),\n",
        "  StructField(\"OrderDate\", DateType(), False),\n",
        "  StructField(\"CustomerName\", StringType(), False),\n",
        "  StructField(\"EmailAddress\", StringType(), False),\n",
        "  StructField(\"Item\", StringType(), False),\n",
        "  StructField(\"Quantity\", IntegerType(), False),\n",
        "  StructField(\"UnitPrice\", StringType(), False),\n",
        "  StructField(\"TaxAmount\", StringType(), False)\n",
        "])\n",
        "\n",
        "#let's not query the entire dataset, instead let's look at the top 20 results\n",
        "OrderDetails = spark.read.csv('/data/*.csv', schema=OrderSchema, header=True)\n",
        "display(OrderDetails.limit(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Infer a Data Schema\n",
        "If the structure of the data source is unknown, you can have Spark automatically infer the schema.\n",
        "\n",
        "In this case, you will load data about airports without knowing the schema.\n",
        "\n",
        "Run the following cell to load airport data from a text file, inferring the column names and data types automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "OrderDetails2019 = spark.read.csv('/data/*.csv', header=True, inferSchema=True)\n",
        "#display(OrderDetails2019.limit(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Count the Rows in a Dataframes\n",
        "Now that you're familiar with working with dataframes, a key task when building predictive solutions is to explore the data, determing statistics that will help you understand the data before building predictive models. For example, how many rows of flight data do you actually have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This spans across all the files to perform a count within the dataframe\n",
        "OrderDetails.count()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Count inferred rows\n",
        "This will execute the same query on inferred data from all of the files with a csv extension in the noted folder. The count should be the same as that above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# This code spans the single file pulled into the dataframe\n",
        "OrderDetails2019.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Remove any headers from the consolidation of the source files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "OrderDetails = OrderDetails.filter(OrderDetails.SalesOrderNumber == \"SalesOrderNumber\")\n",
        "print(OrderDetails)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print('Total rows in dataframe where \\\n",
        "EmailAddress = morgan30@adventure-works.com with where clause')\n",
        "print(OrderDetails.where(OrderDetails.EmailAddress == 'morgan30@adventure-works.com').count())\n",
        "  \n",
        "print('They are  ')\n",
        "OrderDetails.where(OrderDetails.EmailAddress == 'morgan30@adventure-works.com').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Partition the data by OrderDate and Order Item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "OrderDetails.write.partitionBy('OrderDate', 'Item').parquet('OrderDetails')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print('Total rows in dataframe where SalesOrderNumber = SO45347 with where clause')\n",
        "print(OrderDetails.where(OrderDetails.SalesOrderNumber == 'SO45347').count())\n",
        "  \n",
        "print('They are  ')\n",
        "OrderDetails.where(OrderDetails.SalesOrderNumber == 'SO45347').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use Dataframe Methods\n",
        "Spark DataFrames provide functions that you can use to extract and manipulate data. For example, you can use the **select** function to return a new dataframe containing columns selected from an existing dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# the use of dataframes from anoher dataframe is a quick way to create working tables\n",
        "# you will want to manage these as you go along but they allow the data engineer to try different analysis methods easily.\n",
        "cities = airports.select(\"city\", \"name\")\n",
        "display(cities.limit(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine Operations\n",
        "You can combine functions in a single statement to perform multiple operations on a dataframe. In this case, you will use the **join** function to combine the **flights** and **airports** dataframes, and then use the **groupBy** and **count** functions to return the number of flights from each airport, and finally use the **orderBy** function to sort the results by the number of flights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "flightsByOrigin = flights.join(airports, flights.OriginAirportID == airports.airport_id).groupBy(\"city\").count().orderBy(\"count\")\n",
        "display(flightsByOrigin.limit(30))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Determine the Presence of Duplicates\n",
        "The data you have to work with won't always be perfect - often you'll want to *clean* the data; for example to detect and remove duplicates that might affect your model. You can use the **dropDuplicates** function to create a new dataframe with the duplicates removed, enabling you to determine how many rows are duplicates of other rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "flights.count() - flights.dropDuplicates().count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Identify Missing Values\n",
        "As well as determining if duplicates exist in your data, you should detect missing values, and either remove rows containing missing data or replace the missing values with a suitable relacement. The **dropna** function creates a dataframe with any rows containing missing data removed - you can specify a subset of columns, and whether the row should be removed in *any* or *all* values are missing. You can then use this new dataframe to determine how many rows contain missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "flights.count() - flights.dropDuplicates().dropna(how=\"any\", subset=[\"ArrDelay\", \"DepDelay\"]).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clean the Data\n",
        "Now that you've identified that there are duplicates and missing values, you can clean the data by removing the duplicates and replacing the missing values. The **fillna** function replaces missing values with a specified replacement value. In this case, you'll remove all duplicate rows and replace missing **ArrDelay** and **DepDelay** values with **0**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data=flights.dropDuplicates().fillna(value=0, subset=[\"ArrDelay\", \"DepDelay\"])\n",
        "data.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explore the Data\n",
        "Now that you've cleaned the data, you can start to explore it and perform some basic analysis. Let's start by examining the lateness of a flight. The dataset includes the **ArrDelay** field, which tells you how many minutes behind schedule a flight arrived. However, if a flight is only a few minutes behind schedule, you might not consider it *late*. Let's make our definition of lateness such that flights that arrive within 25 minutes of their scheduled arrival time are considered on-time, but any flights that are more than 25 minutes behind schedule are classified as *late*. We'll add a column to indicate this classification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = data.select(\"DayofMonth\", \"DayOfWeek\", \"Carrier\", \"OriginAirportID\",\"DestAirportID\",\n",
        "                   \"DepDelay\", \"ArrDelay\", ((col(\"ArrDelay\") > 25).cast(\"Int\").alias(\"Late\")))\n",
        "display(data.limit(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explore Summary Statistics and Data Distribution\n",
        "Having a good idea of the shape of the data is simple with Spark, you might have a technical spec to compare this too to ensure that your attributes match this technical spec to the actual summary statistics for the columns in our data. The **describe** function returns a dataframe containing the **count**, **mean**, **standard deviation**, **minimum**, and **maximum** values for each numeric column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display(data.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The *DayofMonth* is a value between 1 and 31, and the mean is around halfway between these values; which seems about right. The same is true for the *DayofWeek* which is a value between 1 and 7. *Carrier* is a string, so there are no numeric statistics; and we can ignore the statistics for the airport IDs - they're just unique identifiers for the airports, not actually numeric values. The departure and arrival delays range between 63 or 94 minutes ahead of schedule, and over 1,800 minutes behind schedule. The means are much closer to zero than this, and the standard deviation is quite large; so there's quite a bit of variance in the delays. The *Late* indicator is a 1 or a 0, but the mean is very close to 0; which implies that there significantly fewer late flights than non-late flights.\n",
        "\n",
        "Let's verify that assumption by creating a table and using the **Spark SQL** API to run a SQL statement that counts the number of late and non-late flights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data.createOrReplaceTempView(\"flightData\")\n",
        "lateCounts = spark.sql(\"SELECT COUNT(*) AS Count, Late FROM flightData GROUP BY Late\")\n",
        "display(lateCounts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Yes, it looks like there are significantly more non-late flights than late ones. You can see this more clearly with a visualization, so in the output above, select the **Chart** view to see the comparative counts as a bar chart.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's address the outliers and imbalanced classes in our data by removing rows with extreme delay values, and *undersampling* the more common on-time flights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import rand\n",
        "\n",
        "# Remove outliers - let's make the cut-off 150 minutes.\n",
        "data = data.filter(\"DepDelay < 150 AND ArrDelay < 150\")\n",
        "\n",
        "# Separate the late and on-time flights\n",
        "pos = data.filter(\"Late = 1\")\n",
        "neg = data.filter(\"Late = 0\")\n",
        "\n",
        "# undersample the most prevalent class to get a roughly even distribution\n",
        "posCount = pos.count()\n",
        "negCount = neg.count()\n",
        "if posCount > negCount:\n",
        "  pos = pos.sample(True, negCount/(negCount + posCount))\n",
        "else:\n",
        "  neg = neg.sample(True, posCount/(negCount + posCount))\n",
        "  \n",
        "# shuffle into random order (so a sample of the first 1000 has a mix of classes)\n",
        "data = neg.union(pos).orderBy(rand())\n",
        "\n",
        "# Replace the temporary table so we can query and visualize the balanced dataset\n",
        "data.createOrReplaceTempView(\"flightData\")\n",
        "\n",
        "# Show the statistics\n",
        "display(data.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the maximums for the **DepDelay** and **ArrDelay** are clipped at under 150, and the mean value for the binary *Late* class is nearer 0.5; indicating a more or less even number of each class. We removed some data to accomplish this balancing act, but there are still a substantial number of rows for us to train a machine learning model with, and now the data is more balanced. Let's visualize the data again to confirm this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "microsoft": {
          "language": "sparksql"
        }
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "SELECT Late, COUNT(*) AS Count FROM flightData GROUP BY Late"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display the data as a chart to compare the distribution of the **Late** classes as you did previously. There should now be a more even number of each class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Explore Relationships in the Data\n",
        "Predictive modeling is largely based on statistical relationships between fields in the data. To design a good model, you need to understand how the data points relate to one another.\n",
        "\n",
        "A common way to start exploring relationships is to create visualizations that compare two or more data values. For example, run the following query to compare arrival delay by carrier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "SELECT Carrier, ArrDelay FROM flightData"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "View the output as a chart, and then use the **View Options** button (which looks similar to **&#128463;<sub>*</sub>**) at the top-right of the output to configure the chart as follows:\n",
        "\n",
        "- **Chart type**: Box plot\n",
        "- **Key**: Carrier\n",
        "- **Values**: ArrDelay\n",
        "\n",
        "When you apply the view options, the box plots should should show that the median delay (the line in the middle of the box), and the distribution of delays varies by carrier; with some carriers having a higher median delay than others. The same is true for other features, such as the day of the week and the destination airport."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "You may already suspect that there's likely to be a relationship between departure delay and arrival delay, so let's examine that next. Run the next cell to retrieve the departure and arrival delays for each flight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "SELECT DepDelay, ArrDelay FROM flightData"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "View the utput as a chart, and set the view options as follows:\n",
        "\n",
        "- **Chart type**: Scatter chart\n",
        "- **Key**: DepDelay\n",
        "- **Values**: ArrDelay\n",
        "- **Series Group**: *blank*\n",
        "- **Aggregation**: Avg\n",
        "\n",
        "The scatter plot shows the average arrival deplay for each recorded departure delay. Note that the points form a diagonal line, which indicates a strong linear relationship between departure delay and arrival delay.\n",
        "\n",
        "This linear relationship shows a *correlation* between these two values, which we can measure statistically. The **corr** function calculates a correlation value between -1 and 1, indicating the strength of correlation between two fields. A strong positive correlation (near 1) indicates that high values for one column are often found with high values for the other, which a strong negative correlation (near -1) indicates that *low* values for one column are often found with *high* values for the other. A correlation near 0 indicates little apparent relationship between the fields.\n",
        "\n",
        "Run the following cell to see the correlation statistic for these two variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.corr(\"DepDelay\", \"ArrDelay\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The correlation is close to 1, indicating a reasonably strong positive correlation between departure delay and arrval delay. Flights that depart late, unsurprisingly, often arrive late!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Train a machine learning model\n",
        "\n",
        "In a real scenario, a data scientist would explore the statistical distributions and relationships in the data in more depth, and perform *feature engineering* to prepare the dataset for training a machine learning model. In this exercise, we've provided a prepared version of the data for you. Use the following code to load it into a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "flightSchema = StructType([\n",
        "  StructField(\"DayofMonth\", IntegerType(), False),\n",
        "  StructField(\"DayOfWeek\", IntegerType(), False),\n",
        "  StructField(\"Carrier\", StringType(), False),\n",
        "  StructField(\"OriginAirportID\", IntegerType(), False),\n",
        "  StructField(\"DestAirportID\", IntegerType(), False),\n",
        "  StructField(\"DepDelay\", IntegerType(), False),\n",
        "  StructField(\"ArrDelay\", IntegerType(), False),\n",
        "  StructField(\"Late\", IntegerType(), False),\n",
        "])\n",
        "\n",
        "data = spark.read.csv('/data/flights.csv', schema=flightSchema, header=True)\n",
        "display(data.limit(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "In this notebook, you've explored how to load data from the data lake into a spark notebook, manipulate it and transform it for consumption by other downstream analysts and data scientists.\n",
        "\n",
        "We've only scratched the surface of what can be done with notebooks."
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {
        "97b31a73-c658-4138-8298-9501f762a9de": {
          "persist_state": {
            "view": {
              "chartOptions": {
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "0"
                ],
                "chartType": "bar",
                "isStacked": false,
                "seriesFieldKeys": [
                  "1"
                ]
              },
              "tableOptions": {},
              "type": "details"
            }
          },
          "sync_state": {
            "isSummary": false,
            "language": "scala",
            "table": {
              "rows": [
                {
                  "0": "SO49171",
                  "1": "1",
                  "2": "2021-01-01",
                  "3": "Mariah Foster",
                  "4": "mariah21@adventure-works.com",
                  "5": "Road-250 Black, 48",
                  "6": "1",
                  "7": "2181.5625",
                  "8": "174.525"
                },
                {
                  "0": "SO49172",
                  "1": "1",
                  "2": "2021-01-01",
                  "3": "Brian Howard",
                  "4": "brian23@adventure-works.com",
                  "5": "Road-250 Red, 44",
                  "6": "1",
                  "7": "2443.35",
                  "8": "195.468"
                },
                {
                  "0": "SO49173",
                  "1": "1",
                  "2": "2021-01-01",
                  "3": "Linda Alvarez",
                  "4": "linda19@adventure-works.com",
                  "5": "Mountain-200 Silver, 38",
                  "6": "1",
                  "7": "2071.4196",
                  "8": "165.7136"
                },
                {
                  "0": "SO49174",
                  "1": "1",
                  "2": "2021-01-01",
                  "3": "Gina Hernandez",
                  "4": "gina4@adventure-works.com",
                  "5": "Mountain-200 Silver, 42",
                  "6": "1",
                  "7": "2071.4196",
                  "8": "165.7136"
                },
                {
                  "0": "SO49178",
                  "1": "1",
                  "2": "2021-01-01",
                  "3": "Beth Ruiz",
                  "4": "beth4@adventure-works.com",
                  "5": "Road-550-W Yellow, 44",
                  "6": "1",
                  "7": "1000.4375",
                  "8": "80.035"
                },
                {
                  "0": "SO49179",
                  "1": "1",
                  "2": "2021-01-01",
                  "3": "Evan Ward",
                  "4": "evan13@adventure-works.com",
                  "5": "Road-550-W Yellow, 38",
                  "6": "1",
                  "7": "1000.4375",
                  "8": "80.035"
                },
                {
                  "0": "SO49175",
                  "1": "1",
                  "2": "2021-01-01",
                  "3": "Margaret Guo",
                  "4": "margaret24@adventure-works.com",
                  "5": "Road-250 Red, 52",
                  "6": "1",
                  "7": "2443.35",
                  "8": "195.468"
                },
                {
                  "0": "SO49180",
                  "1": "1",
                  "2": "2021-01-01",
                  "3": "Mitchell Yuan",
                  "4": "mitchell6@adventure-works.com",
                  "5": "Road-650 Black, 58",
                  "6": "1",
                  "7": "782.99",
                  "8": "62.6392"
                },
                {
                  "0": "SO49176",
                  "1": "1",
                  "2": "2021-01-01",
                  "3": "Shawn Sharma",
                  "4": "shawn11@adventure-works.com",
                  "5": "Mountain-200 Silver, 46",
                  "6": "1",
                  "7": "2071.4196",
                  "8": "165.7136"
                },
                {
                  "0": "SO49177",
                  "1": "1",
                  "2": "2021-01-01",
                  "3": "Barbara Chande",
                  "4": "barbara44@adventure-works.com",
                  "5": "Mountain-200 Silver, 42",
                  "6": "1",
                  "7": "2071.4196",
                  "8": "165.7136"
                },
                {
                  "0": "SO49186",
                  "1": "1",
                  "2": "2021-01-02",
                  "3": "Cara Xu",
                  "4": "cara8@adventure-works.com",
                  "5": "Road-250 Red, 52",
                  "6": "1",
                  "7": "2443.35",
                  "8": "195.468"
                },
                {
                  "0": "SO49187",
                  "1": "1",
                  "2": "2021-01-02",
                  "3": "Lacey Liu",
                  "4": "lacey16@adventure-works.com",
                  "5": "Road-250 Black, 58",
                  "6": "1",
                  "7": "2181.5625",
                  "8": "174.525"
                },
                {
                  "0": "SO49190",
                  "1": "1",
                  "2": "2021-01-02",
                  "3": "Omar Zhu",
                  "4": "omar13@adventure-works.com",
                  "5": "Road-550-W Yellow, 40",
                  "6": "1",
                  "7": "1000.4375",
                  "8": "80.035"
                },
                {
                  "0": "SO49185",
                  "1": "1",
                  "2": "2021-01-02",
                  "3": "Cassandra Fernandez",
                  "4": "cassandra17@adventure-works.com",
                  "5": "Mountain-200 Black, 46",
                  "6": "1",
                  "7": "2049.0982",
                  "8": "163.9279"
                },
                {
                  "0": "SO49184",
                  "1": "1",
                  "2": "2021-01-02",
                  "3": "Monica Martinez",
                  "4": "monica17@adventure-works.com",
                  "5": "Mountain-200 Black, 46",
                  "6": "1",
                  "7": "2049.0982",
                  "8": "163.9279"
                },
                {
                  "0": "SO49189",
                  "1": "1",
                  "2": "2021-01-02",
                  "3": "Marie Gonzalez",
                  "4": "marie20@adventure-works.com",
                  "5": "Road-650 Black, 48",
                  "6": "1",
                  "7": "782.99",
                  "8": "62.6392"
                },
                {
                  "0": "SO49182",
                  "1": "1",
                  "2": "2021-01-02",
                  "3": "Alexandra Hall",
                  "4": "alexandra89@adventure-works.com",
                  "5": "Road-250 Red, 48",
                  "6": "1",
                  "7": "2443.35",
                  "8": "195.468"
                },
                {
                  "0": "SO49183",
                  "1": "1",
                  "2": "2021-01-02",
                  "3": "Alejandro Raji",
                  "4": "alejandro46@adventure-works.com",
                  "5": "Road-250 Red, 52",
                  "6": "1",
                  "7": "2443.35",
                  "8": "195.468"
                },
                {
                  "0": "SO49181",
                  "1": "1",
                  "2": "2021-01-02",
                  "3": "Derrick Jim�nez",
                  "4": "derrick5@adventure-works.com",
                  "5": "Road-250 Black, 48",
                  "6": "1",
                  "7": "2181.5625",
                  "8": "174.525"
                },
                {
                  "0": "SO49188",
                  "1": "1",
                  "2": "2021-01-02",
                  "3": "Erin Cox",
                  "4": "erin15@adventure-works.com",
                  "5": "Mountain-200 Black, 38",
                  "6": "1",
                  "7": "2049.0982",
                  "8": "163.9279"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "SalesOrderNumber",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "SalesOrderLineNumber",
                  "type": "int"
                },
                {
                  "key": "2",
                  "name": "OrderDate",
                  "type": "date"
                },
                {
                  "key": "3",
                  "name": "CustomerName",
                  "type": "string"
                },
                {
                  "key": "4",
                  "name": "EmailAddress",
                  "type": "string"
                },
                {
                  "key": "5",
                  "name": "Item",
                  "type": "string"
                },
                {
                  "key": "6",
                  "name": "Quantity",
                  "type": "int"
                },
                {
                  "key": "7",
                  "name": "UnitPrice",
                  "type": "string"
                },
                {
                  "key": "8",
                  "name": "TaxAmount",
                  "type": "string"
                }
              ],
              "truncated": false
            }
          },
          "type": "Synapse.DataFrame"
        }
      },
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
