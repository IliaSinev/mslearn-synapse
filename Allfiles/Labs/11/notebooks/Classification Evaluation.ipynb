{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating a Classification Model\n",
        "\n",
        "In this exercise, you will create a pipeline for a classification model, and then apply commonly used metrics to evaluate the resulting classifier.\n",
        "\n",
        "### Prepare the Data\n",
        "\n",
        "First, import the libraries you will need and prepare the training and test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, MinMaxScaler\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Load the source data\n",
        "flightSchema = StructType([\n",
        "  StructField(\"DayofMonth\", IntegerType(), False),\n",
        "  StructField(\"DayOfWeek\", IntegerType(), False),\n",
        "  StructField(\"Carrier\", StringType(), False),\n",
        "  StructField(\"OriginAirportID\", StringType(), False),\n",
        "  StructField(\"DestAirportID\", StringType(), False),\n",
        "  StructField(\"DepDelay\", IntegerType(), False),\n",
        "  StructField(\"ArrDelay\", IntegerType(), False),\n",
        "  StructField(\"Late\", IntegerType(), False),\n",
        "])\n",
        "\n",
        "data = spark.read.csv('wasb://spark@<YOUR_ACCOUNT>.blob.core.windows.net/data/flights.csv', schema=flightSchema, header=True)\n",
        "\n",
        "# Split the data\n",
        "splits = data.randomSplit([0.7, 0.3])\n",
        "train = splits[0]\n",
        "test = splits[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the Pipeline and Train the Model\n",
        "Now define a pipeline that creates a feature vector and trains a classification model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "scrolled": false
      },
      "source": [
        "monthdayIndexer = StringIndexer(inputCol=\"DayofMonth\", outputCol=\"DayofMonthIdx\")\n",
        "weekdayIndexer = StringIndexer(inputCol=\"DayOfWeek\", outputCol=\"DayOfWeekIdx\")\n",
        "carrierIndexer = StringIndexer(inputCol=\"Carrier\", outputCol=\"CarrierIdx\")\n",
        "originIndexer = StringIndexer(inputCol=\"OriginAirportID\", outputCol=\"OriginAirportIdx\")\n",
        "destIndexer = StringIndexer(inputCol=\"DestAirportID\", outputCol=\"DestAirportIdx\")\n",
        "numVect = VectorAssembler(inputCols = [\"DepDelay\"], outputCol=\"numFeatures\")\n",
        "minMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol=\"normNums\")\n",
        "featVect = VectorAssembler(inputCols=[\"DayofMonthIdx\", \"DayOfWeekIdx\", \"CarrierIdx\", \"OriginAirportIdx\", \"DestAirportIdx\", \"normNums\"], outputCol=\"features\")\n",
        "lr = LogisticRegression(labelCol=\"Late\", featuresCol=\"features\")\n",
        "pipeline = Pipeline(stages=[monthdayIndexer, weekdayIndexer, carrierIndexer, originIndexer, destIndexer, numVect, minMax, featVect, lr])\n",
        "model = pipeline.fit(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the Model\n",
        "Now you're ready to apply the model to the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "scrolled": false
      },
      "source": [
        "prediction = model.transform(test)\n",
        "predicted = prediction.select(\"features\", col(\"prediction\").cast(\"Int\"), col(\"Late\").alias(\"trueLabel\"))\n",
        "predicted.show(100, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute Confusion Matrix Metrics\n",
        "Classifiers are typically evaluated by creating a *confusion matrix*, which indicates the number of:\n",
        "- True Positives\n",
        "- True Negatives\n",
        "- False Positives\n",
        "- False Negatives\n",
        "\n",
        "From these core measures, other evaluation metrics such as *precision* and *recall* can be calculated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "tp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 1\").count())\n",
        "fp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 0\").count())\n",
        "tn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 0\").count())\n",
        "fn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 1\").count())\n",
        "metrics = spark.createDataFrame([\n",
        " (\"TP\", tp),\n",
        " (\"FP\", fp),\n",
        " (\"TN\", tn),\n",
        " (\"FN\", fn),\n",
        " (\"Precision\", tp / (tp + fp)),\n",
        " (\"Recall\", tp / (tp + fn))],[\"metric\", \"value\"])\n",
        "metrics.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### View the Raw Prediction and Probability\n",
        "The prediction is based on a raw prediction score that describes a labelled point in a logistic function. This raw prediction is then converted to a predicted label of 0 or 1 based on a probability vector that indicates the confidence for each possible label value (in this case, 0 and 1). The value with the highest confidence is selected as the prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {},
      "source": [
        "prediction.select(\"rawPrediction\", \"probability\", \"prediction\", col(\"Late\").alias(\"trueLabel\")).show(100, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the results include rows where the probability for 0 (the first value in the **probability** vector) is only slightly higher than the probability for 1 (the second value in the **probability** vector). The default *discrimination threshold* (the boundary that decides whether a probability is predicted as a 1 or a 0) is set to 0.5; so the prediction with the highest probability is always used, no matter how close to the threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Review the Area Under ROC\n",
        "Another way to assess the performance of a classification model is to measure the area under a *received operator characteristic (ROC) curve* for the model. The **spark.ml** library includes a **BinaryClassificationEvaluator** class that you can use to compute this. A ROC curve plots the True Positive and False Positive rates for varying threshold values (the probability value over which a class label is predicted). The area under this curve gives an overall indication of the models accuracy as a value between 0 and 1. A value under 0.5 means that a binary classification model (which predicts one of two possible labels) is no better at predicting the right class than a random 50/50 guess."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {},
      "source": [
        "evaluator = BinaryClassificationEvaluator(labelCol=\"Late\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "auc = evaluator.evaluate(prediction)\n",
        "print (\"AUC = \", auc)"
      ]
    }
  ]
}